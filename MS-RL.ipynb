{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddeb07e-d4f5-4edc-b36e-fa6268970ff7",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f5352-025f-4a78-8c63-4712ec0a8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "c3f=pd.read_csv(r\"C:\\Users\\Kaveri\\Downloads\\CHASE datasets\\c3f.csv\")\n",
    "iris_c=pd.read_csv(r\"C:\\Users\\Kaveri\\Downloads\\ds-researcch\\iris_c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6dcd6-31f6-4758-a817-e4ede0e852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def load_digits_dataset():\n",
    "    digits = load_digits(as_frame=True)   # returns DataFrame + Series\n",
    "    X = digits.data        # features (1797 × 64)\n",
    "    y = digits.target      # labels (1797,)\n",
    "    return X, y\n",
    "\n",
    "# Example usage\n",
    "X_digits, y_digits = load_digits_dataset()\n",
    "print(X_digits.shape, y_digits.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544414d4-1c38-4c2f-963a-0ec125081dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "\n",
    "def load_rcv1_dataset():\n",
    "    rcv1 = fetch_rcv1()\n",
    "    X = rcv1.data        # sparse TF-IDF matrix (804k × 47k)\n",
    "    y = rcv1.target      # multilabel (804k × 103)\n",
    "    target_names = rcv1.target_names\n",
    "    return X, y, target_names\n",
    "\n",
    "# Example usage\n",
    "X_rcv1, y_rcv1, labels_rcv1 = load_rcv1_dataset()\n",
    "print(X_rcv1.shape, y_rcv1.shape, labels_rcv1[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f9ad8-af2d-4c11-b7b6-f1fcdfaa7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups, fetch_20newsgroups_vectorized\n",
    "\n",
    "def load_20news_dataset(vectorized=True):\n",
    "    if vectorized:\n",
    "        # Already TF-IDF features\n",
    "        newsgroups = fetch_20newsgroups_vectorized(subset='all')\n",
    "        X = newsgroups.data      # sparse matrix (18846 × 130k)\n",
    "        y = newsgroups.target    # class labels (0–19)\n",
    "        target_names = newsgroups.target_names\n",
    "    else:\n",
    "        # Raw text version\n",
    "        newsgroups = fetch_20newsgroups(subset='all')\n",
    "        X = newsgroups.data\n",
    "        y = newsgroups.target\n",
    "        target_names = newsgroups.target_names\n",
    "    \n",
    "    return X, y, target_names\n",
    "\n",
    "# Example usage\n",
    "X_20news, y_20news, labels_20news = load_20news_dataset(vectorized=True)\n",
    "print(X_20news.shape, len(set(y_20news)), labels_20news[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3dbc4-1e3a-4c64-8620-d8fcc67eb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8925da0-b5cd-4dce-8e59-77ac2ac5f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "\n",
    "digits = load_digits(as_frame=True)\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(\"DIGITS DATASET\")\n",
    "print(\"Shape:\", X_digits.shape)\n",
    "print(\"Classes:\", y_digits.nunique())\n",
    "print(y_digits.value_counts())\n",
    "\n",
    "# Show first 5 rows\n",
    "display(X_digits.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aff6a7-2cfe-4743-b219-a1426277f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "import pandas as pd\n",
    "\n",
    "news = fetch_20newsgroups_vectorized(subset=\"all\")\n",
    "X_20news, y_20news = news.data, news.target\n",
    "\n",
    "print(\"\\n20 NEWSGROUPS DATASET (preview 5 docs × 10 features)\")\n",
    "df_20news = pd.DataFrame.sparse.from_spmatrix(X_20news[:5, :10])  # first 5 docs, 10 features\n",
    "display(df_20news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34949d7-db77-4944-8e97-bcc3a4b2fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "import pandas as pd\n",
    "\n",
    "rcv1 = fetch_rcv1()\n",
    "X_rcv1, y_rcv1 = rcv1.data, rcv1.target\n",
    "\n",
    "print(\"\\nRCV1 DATASET (preview 5 docs × 10 features)\")\n",
    "df_rcv1 = pd.DataFrame.sparse.from_spmatrix(X_rcv1[:5, :10])  # first 5 docs, 10 features\n",
    "display(df_rcv1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1361eba-f4ef-493b-b2b5-94485c2c0c63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocess datasets into (X, y) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eee27-4bb0-46e9-985b-6aa1c2ccc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Take y_rcv1 (sparse matrix, 804k × 103)\n",
    "# Convert to array of topic indices: choose the *first* label per doc\n",
    "y_rcv1_single = y_rcv1.argmax(axis=1).A1   # pick index of max label (first positive)\n",
    "\n",
    "print(\"RCV1 single-label shape:\", y_rcv1_single.shape)\n",
    "print(\"Unique classes:\", np.unique(y_rcv1_single)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db3fd7-6f79-416d-b32c-c6aeadc52af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already single-label multiclass\n",
    "y_20news_single = y_20news  # just rename for consistency\n",
    "print(\"20NEWS unique classes:\", len(set(y_20news_single)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365830c-a045-4898-bcee-d4bfa172a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already fine #digits\n",
    "y_digits_single = y_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd69bd-8922-48d5-87e5-733ec9485932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(c3f.head())\n",
    "print(c3f.columns)\n",
    "\n",
    "# Suppose last 3 columns are targets: Cat, Sub_Cat, Label\n",
    "# (adjust names if different)\n",
    "target_cols = [\"Cat\", \"Sub_Cat\", \"Label\"]\n",
    "\n",
    "# Features = all except target columns\n",
    "X_c3f = c3f.drop(columns=target_cols)\n",
    "\n",
    "# Targets = separate Series for each level\n",
    "y_c3f_cat = c3f[\"Cat\"]\n",
    "y_c3f_sub = c3f[\"Sub_Cat\"]\n",
    "y_c3f_label = c3f[\"Label\"]\n",
    "\n",
    "print(\"Features:\", X_c3f.shape, \"Cat:\", y_c3f_cat.nunique(),\n",
    "      \"SubCat:\", y_c3f_sub.nunique(), \"Label:\", y_c3f_label.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce4049-60d2-4f42-9ef4-5e725c1cd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_c = iris_c.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933775f-c37f-4b85-9174-6bf17ec14f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_wide = iris_c.pivot_table(\n",
    "    index=\"Species\",              # group by species\n",
    "    columns=[\"Part\", \"Measure\"],  # create feature columns\n",
    "    values=\"Value\", \n",
    "    aggfunc=\"mean\"                # average if duplicates\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b04cd-b6ba-4606-bf2a-ed192455b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iris = iris_wide[\"Species\"]              # target\n",
    "X_iris = iris_wide.drop(columns=[\"Species\"])  # features\n",
    "\n",
    "print(X_iris.shape, y_iris.nunique(), y_iris.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a265151-653c-4266-9609-6a55c5a3254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4d7cc-8966-4a05-a8cd-00dc29c0a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Encode species into integers\n",
    "le_iris = LabelEncoder()\n",
    "y_iris_encoded = le_iris.fit_transform(y_iris)\n",
    "\n",
    "print(\"Iris Classes:\", le_iris.classes_)\n",
    "print(\"Sample Encoded Labels:\", np.unique(y_iris_encoded)[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19875d-fb81-45be-9a36-a32f122ba6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Unnamed: 0 still exists, drop it safely\n",
    "if \"Unnamed: 0\" in iris_c.columns:\n",
    "    iris_c = iris_c.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Pivot into wide format: each row = one flower\n",
    "iris_wide = iris_c.pivot_table(\n",
    "    index=iris_c.index,             # keep row identity\n",
    "    columns=[\"Part\", \"Measure\"],    # make sepal_length, sepal_width, etc.\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# Add Species back as target\n",
    "iris_wide[\"Species\"] = iris_c[\"Species\"]\n",
    "\n",
    "# Features and target\n",
    "X_iris = iris_wide.drop(columns=[\"Species\"])\n",
    "y_iris = iris_wide[\"Species\"]\n",
    "\n",
    "# Encode target labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_iris = LabelEncoder()\n",
    "y_iris_encoded = le_iris.fit_transform(y_iris)\n",
    "\n",
    "print(\"X shape:\", X_iris.shape)\n",
    "print(\"y length:\", len(y_iris_encoded))\n",
    "print(\"Classes:\", le_iris.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8977f38-a703-4468-8ba4-467f93cff70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artificial sample_id (every 4 rows = 1 sample)\n",
    "iris_c = iris_c.copy()\n",
    "iris_c[\"sample_id\"] = iris_c.index // 4\n",
    "\n",
    "# Pivot into wide format\n",
    "iris_wide = iris_c.pivot_table(\n",
    "    index=\"sample_id\",\n",
    "    columns=[\"Part\", \"Measure\"],\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# Add target back (species) - take the first value per sample_id\n",
    "iris_wide[\"Species\"] = iris_c.groupby(\"sample_id\")[\"Species\"].first()\n",
    "\n",
    "# Features and target\n",
    "X_iris = iris_wide.drop(columns=[\"Species\"])\n",
    "y_iris = iris_wide[\"Species\"]\n",
    "\n",
    "# Encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_iris = LabelEncoder()\n",
    "y_iris_encoded = le_iris.fit_transform(y_iris)\n",
    "\n",
    "print(\"X shape:\", X_iris.shape)\n",
    "print(\"y length:\", len(y_iris_encoded))\n",
    "print(\"Classes:\", le_iris.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2a112-24c7-4a62-84f4-f6277a6b34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode Cat\n",
    "le_cat = LabelEncoder()\n",
    "y_c3f_cat_enc = le_cat.fit_transform(y_c3f_cat)\n",
    "\n",
    "# Encode Sub_Cat\n",
    "le_sub = LabelEncoder()\n",
    "y_c3f_sub_enc = le_sub.fit_transform(y_c3f_sub)\n",
    "\n",
    "# Encode Label\n",
    "le_label = LabelEncoder()\n",
    "y_c3f_label_enc = le_label.fit_transform(y_c3f_label)\n",
    "\n",
    "print(\"Cat classes:\", le_cat.classes_)\n",
    "print(\"SubCat classes:\", len(le_sub.classes_))\n",
    "print(\"Label classes:\", len(le_label.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d2c89-16fc-4d7c-a560-df22cbf5035f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## computing complexity/meta-features for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd39f70-706b-42aa-9bd9-be32cac79641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class DataComplexityAnalyzer:\n",
    "    \"\"\"\n",
    "    Calculates data complexity measures (Ho & Basu + extensions).\n",
    "    \"\"\"\n",
    "\n",
    "    def safe_cv(self, y, max_cv=3):\n",
    "        \"\"\"\n",
    "        Choose number of CV folds based on minimum class size.\n",
    "        If any class has <2 samples, return None (skip CV).\n",
    "        \"\"\"\n",
    "        counts = np.bincount(y)\n",
    "        min_class_size = np.min(counts)\n",
    "        if min_class_size < 2:\n",
    "            return None  # too few samples for CV\n",
    "        return min(max_cv, min_class_size)\n",
    "\n",
    "    def calculate_n1_borderline_points(self, X, y):\n",
    "        if len(X) < 6: \n",
    "            return 0.0\n",
    "        k = min(6, len(X))\n",
    "        nn = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(X)\n",
    "        _, indices = nn.kneighbors(X)\n",
    "        borderline = sum(len(np.unique(y[indices[i][1:]])) > 1 for i in range(len(X)))\n",
    "        return borderline / len(X)\n",
    "\n",
    "    def calculate_n2_intra_class_ratio(self, X, y):\n",
    "        if len(X) < 4: \n",
    "            return 1.0\n",
    "        nn = NearestNeighbors(n_neighbors=2, metric='euclidean').fit(X)\n",
    "        dists, indices = nn.kneighbors(X)\n",
    "        intra, inter = [], []\n",
    "        for i in range(len(X)):\n",
    "            ni, d = indices[i][1], dists[i][1]\n",
    "            (intra if y[i] == y[ni] else inter).append(d)\n",
    "        return np.mean(intra)/np.mean(inter) if intra and inter else 1.0\n",
    "\n",
    "    def calculate_n3_error_rate(self, X, y):\n",
    "        if len(X) < 3: \n",
    "            return 0.5\n",
    "        errors = 0\n",
    "        for i in range(len(X)):\n",
    "            X_train, y_train = np.delete(X, i, 0), np.delete(y, i)\n",
    "            nn = NearestNeighbors(n_neighbors=1).fit(X_train)\n",
    "            pred = y_train[nn.kneighbors(X[i].reshape(1, -1))[1][0][0]]\n",
    "            errors += (pred != y[i])\n",
    "        return errors / len(X)\n",
    "\n",
    "    def calculate_l2_linear_separability(self, X, y):\n",
    "        if len(X) < 10: \n",
    "            return 0.5\n",
    "        cv = self.safe_cv(y)\n",
    "        if cv is None: \n",
    "            return 0.5\n",
    "        clf = LogisticRegression(max_iter=200)\n",
    "        return 1 - np.mean(cross_val_score(clf, X, y, cv=cv))\n",
    "\n",
    "    def calculate_l3_nonlinear_separability(self, X, y):\n",
    "        if len(X) < 10: \n",
    "            return 0.5\n",
    "        cv = self.safe_cv(y)\n",
    "        if cv is None: \n",
    "            return 0.5\n",
    "        clf = DecisionTreeClassifier(max_depth=5)\n",
    "        return 1 - np.mean(cross_val_score(clf, X, y, cv=cv))\n",
    "\n",
    "    def calculate_f1_feature_efficiency(self, X, y):\n",
    "        cv = self.safe_cv(y)\n",
    "        if cv is None: \n",
    "            return 0.5\n",
    "        max_eff = 0\n",
    "        for j in range(X.shape[1]):\n",
    "            clf = DecisionTreeClassifier(max_depth=1)\n",
    "            scores = cross_val_score(clf, X[:, j].reshape(-1, 1), y, cv=cv)\n",
    "            max_eff = max(max_eff, np.mean(scores))\n",
    "        return 1 - max_eff\n",
    "\n",
    "    def calculate_all_measures(self, X, y, target_name=\"\"):\n",
    "        if isinstance(X, pd.DataFrame): \n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series): \n",
    "            y = y.values\n",
    "        if not np.issubdtype(y.dtype, np.integer):\n",
    "            y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        measures = {\n",
    "            \"n1_borderline\": self.calculate_n1_borderline_points(X, y),\n",
    "            \"n2_ratio\": self.calculate_n2_intra_class_ratio(X, y),\n",
    "            \"n3_error\": self.calculate_n3_error_rate(X, y),\n",
    "            \"l2_linear_sep\": self.calculate_l2_linear_separability(X, y),\n",
    "            \"l3_nonlinear_sep\": self.calculate_l3_nonlinear_separability(X, y),\n",
    "            \"f1_feature_eff\": self.calculate_f1_feature_efficiency(X, y),\n",
    "            \"num_classes\": len(np.unique(y)),\n",
    "            \"num_features\": X.shape[1],\n",
    "            \"num_samples\": X.shape[0]\n",
    "        }\n",
    "        return measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04731fdd-dab2-4031-8dc7-2a90ed5b776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = DataComplexityAnalyzer()\n",
    "\n",
    "iris_measures = analyzer.calculate_all_measures(X_iris.values, y_iris_encoded, target_name=\"iris\")\n",
    "\n",
    "print(\"\\nIris Meta-Features:\")\n",
    "for k, v in iris_measures.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459a439-2a4c-4776-8eba-4dd0b6ad930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_iris.shape, len(y_iris_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ea2ef-9105-4fcb-93c8-13d383e56f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load Digits\n",
    "digits = load_digits(as_frame=True)\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(\"Digits shape:\", X_digits.shape, \"Classes:\", len(np.unique(y_digits)))\n",
    "\n",
    "# Run analyzer\n",
    "digits_measures = analyzer.calculate_all_measures(X_digits.values, y_digits, target_name=\"digits\")\n",
    "\n",
    "print(\"\\nDigits Meta-Features:\")\n",
    "for k, v in digits_measures.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6041a-a710-487d-9e96-cad5369be464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load RCV1\n",
    "rcv1 = fetch_rcv1()\n",
    "X_rcv1, y_rcv1 = rcv1.data, rcv1.target\n",
    "\n",
    "# Convert multi-label -> single-label (pick first label per sample)\n",
    "y_rcv1_single = y_rcv1.argmax(axis=1).A1  \n",
    "\n",
    "print(\"RCV1 full shape:\", X_rcv1.shape, \"Unique classes:\", len(np.unique(y_rcv1_single)))\n",
    "\n",
    "# Use only first 1000 samples\n",
    "X_rcv1_small = X_rcv1[:1000]\n",
    "y_rcv1_small = y_rcv1_single[:1000]\n",
    "\n",
    "# Reduce dimensionality with SVD (from 47k → 200)\n",
    "svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "X_rcv1_reduced = svd.fit_transform(X_rcv1_small)\n",
    "\n",
    "print(\"Subset shape after reduction:\", X_rcv1_reduced.shape)\n",
    "\n",
    "# Run analyzer\n",
    "rcv1_measures = analyzer.calculate_all_measures(X_rcv1_reduced, y_rcv1_small, target_name=\"rcv1_small\")\n",
    "\n",
    "print(\"\\nRCV1 Meta-Features (subset of 1000, reduced to 200 features):\")\n",
    "for k, v in rcv1_measures.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b610a-a4fc-47ba-8eec-c708a000973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load vectorized 20 Newsgroups (TF-IDF)\n",
    "news = fetch_20newsgroups_vectorized(subset=\"all\")\n",
    "X_20news, y_20news = news.data, news.target\n",
    "target_names_20news = news.target_names\n",
    "\n",
    "print(\"20 Newsgroups full shape:\", X_20news.shape, \"Unique classes:\", len(np.unique(y_20news)))\n",
    "\n",
    "# Take only first 500 samples\n",
    "X_20news_small = X_20news[:500]\n",
    "y_20news_small = y_20news[:500]\n",
    "\n",
    "# Reduce dimensionality with TruncatedSVD (130k → 200 features)\n",
    "svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "X_20news_reduced = svd.fit_transform(X_20news_small)\n",
    "\n",
    "print(\"Subset shape after reduction:\", X_20news_reduced.shape)\n",
    "\n",
    "# Run analyzer\n",
    "news_measures = analyzer.calculate_all_measures(X_20news_reduced, y_20news_small, target_name=\"20newsgroups_small\")\n",
    "\n",
    "print(\"\\n20 Newsgroups Meta-Features (subset of 500, reduced to 200 features):\")\n",
    "for k, v in news_measures.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd6d86-a2e9-4f19-81c4-b28b1214cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to float\n",
    "X_c3f = X_c3f.astype(float)\n",
    "\n",
    "# Replace inf / -inf with NaN\n",
    "X_c3f = X_c3f.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Replace NaN with column mean (or median)\n",
    "X_c3f = X_c3f.fillna(X_c3f.mean())\n",
    "\n",
    "# Optional: clip extremely large values to a safe range\n",
    "X_c3f = X_c3f.clip(lower=-1e6, upper=1e6)\n",
    "\n",
    "print(\"Any NaN left?\", X_c3f.isna().sum().sum())\n",
    "print(\"Any inf left?\", np.isinf(X_c3f.values).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486d5be-2d41-402d-8624-3d7fe81e5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey-patch: redefine calculate_n3_error_rate only for this analyzer instance\n",
    "def fast_n3_error_rate(self, X, y):\n",
    "    # Skip heavy leave-one-out when dataset is large\n",
    "    if X.shape[0] > 500:\n",
    "        return 0.5   # neutral default\n",
    "    else:\n",
    "        return DataComplexityAnalyzer.calculate_n3_error_rate(self, X, y)  # original method\n",
    "\n",
    "# Apply patch\n",
    "analyzer.calculate_n3_error_rate = fast_n3_error_rate.__get__(analyzer, DataComplexityAnalyzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f020e-03fd-4460-badb-259e7dfa6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress only LogisticRegression convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bc3c0-beac-45fe-91f9-74b1b24ad479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller sample size (e.g. 500) so other measures finish quickly\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_c3f_small, _, y_cat_small, _ = train_test_split(\n",
    "    X_c3f, y_c3f_cat_enc, train_size=500, stratify=y_c3f_cat_enc, random_state=42\n",
    ")\n",
    "_, _, y_sub_small, _ = train_test_split(\n",
    "    X_c3f, y_c3f_sub_enc, train_size=500, stratify=y_c3f_sub_enc, random_state=42\n",
    ")\n",
    "_, _, y_label_small, _ = train_test_split(\n",
    "    X_c3f, y_c3f_label_enc, train_size=500, stratify=y_c3f_label_enc, random_state=42\n",
    ")\n",
    "\n",
    "cat_measures   = analyzer.calculate_all_measures(X_c3f_small.values, y_cat_small,   target_name=\"c3f_cat\")\n",
    "sub_measures   = analyzer.calculate_all_measures(X_c3f_small.values, y_sub_small,   target_name=\"c3f_sub\")\n",
    "label_measures = analyzer.calculate_all_measures(X_c3f_small.values, y_label_small, target_name=\"c3f_label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5433416-7fbe-4e2b-8913-206b69d06cac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0adf83-e74b-4e5d-9815-e0faba6c667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\n",
    "    \"xgb+lgb\",\n",
    "    \"xgb\",\n",
    "    \"lgb\",\n",
    "    \"xgb+rf\",\n",
    "    \"lgb+rf\",\n",
    "    \"xgb+lgb+rf\",\n",
    "    \"xgb+lgb+rf_meta\",\n",
    "    \"xgb+lgb_meta\",\n",
    "    \"xgb+rf_meta\",\n",
    "    \"lgb+rf_meta\",\n",
    "    \"logistic\",\n",
    "    \"rf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e979a-2d21-48d6-9b6b-f884208e31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {\n",
    "    0: {\"models\": [\"XGBoost\", \"LightGBM\"], \"meta\": None},\n",
    "    1: {\"models\": [\"XGBoost\"], \"meta\": None},\n",
    "    2: {\"models\": [\"LightGBM\"], \"meta\": None},\n",
    "    3: {\"models\": [\"XGBoost\", \"RandomForest\"], \"meta\": None},\n",
    "    4: {\"models\": [\"LightGBM\", \"RandomForest\"], \"meta\": None},\n",
    "    5: {\"models\": [\"XGBoost\", \"LightGBM\", \"RandomForest\"], \"meta\": None},\n",
    "    6: {\"models\": [\"XGBoost\", \"LightGBM\", \"RandomForest\"], \"meta\": \"LogisticRegression\"},\n",
    "    7: {\"models\": [\"XGBoost\", \"LightGBM\"], \"meta\": \"LogisticRegression\"},\n",
    "    8: {\"models\": [\"XGBoost\", \"RandomForest\"], \"meta\": \"LogisticRegression\"},\n",
    "    9: {\"models\": [\"LightGBM\", \"RandomForest\"], \"meta\": \"LogisticRegression\"},\n",
    "    10: {\"models\": [\"LogisticRegression\"], \"meta\": None},\n",
    "    11: {\"models\": [\"RandomForest\"], \"meta\": None}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69d5ea-95ce-49de-9e3a-ff9325a1b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_id = 3  # example choice\n",
    "config = action_map[action_id]\n",
    "print(\"Chosen models:\", config[\"models\"])\n",
    "print(\"Meta-learner:\", config[\"meta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4c3bd-7f9a-48fc-860f-011b6a6bc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analyzer\n",
    "digits_measures = analyzer.calculate_all_measures(X_digits.values, y_digits, target_name=\"digits\")\n",
    "\n",
    "# Define which features are part of the RL state\n",
    "state_keys = [\n",
    "    \"n1_borderline\",\n",
    "    \"n2_ratio\",\n",
    "    \"n3_error\",\n",
    "    \"l2_linear_sep\",\n",
    "    \"l3_nonlinear_sep\",\n",
    "    \"f1_feature_eff\",\n",
    "    \"num_classes\",\n",
    "    \"num_features\",\n",
    "    \"num_samples\"\n",
    "]\n",
    "\n",
    "# Create state vector\n",
    "state = np.array([digits_measures[k] for k in state_keys], dtype=float)\n",
    "print(\"RL State vector (Digits):\", state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a84ad-4897-4f67-823e-580dd1e3fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_state(measures, state_keys=None):\n",
    "    if state_keys is None:\n",
    "        state_keys = [\n",
    "            \"n1_borderline\", \"n2_ratio\", \"n3_error\",\n",
    "            \"l2_linear_sep\", \"l3_nonlinear_sep\", \"f1_feature_eff\",\n",
    "            \"num_classes\", \"num_features\", \"num_samples\"\n",
    "        ]\n",
    "    return np.array([measures[k] for k in state_keys], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7c7d1-fac0-4b11-bf18-7a132c003332",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_digits = extract_state(digits_measures)\n",
    "print(state_digits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376fd8b-a408-4879-a7d0-f2e8d4ee14a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0522cdc-9685-4ac7-8a2a-f86b1d721b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "\n",
    "def evaluate_action(X, y, action_id, action_map, test_size=0.2):\n",
    "    # Split train/test\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "    \n",
    "    # Get config for this action\n",
    "    config = action_map[action_id]\n",
    "    models = config[\"models\"]\n",
    "    meta = config[\"meta\"]\n",
    "    \n",
    "    # Build estimators\n",
    "    estimators = []\n",
    "    for m in models:\n",
    "        if m == \"RandomForest\":\n",
    "            estimators.append((m, RandomForestClassifier(random_state=42)))\n",
    "        elif m == \"XGBoost\":\n",
    "            estimators.append((m, XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)))\n",
    "        elif m == \"LightGBM\":\n",
    "            estimators.append((m, LGBMClassifier(random_state=42)))\n",
    "        elif m == \"LogisticRegression\":\n",
    "            estimators.append((m, LogisticRegression(max_iter=500, random_state=42)))\n",
    "    \n",
    "    # Meta-learning (stacking)\n",
    "    if meta:\n",
    "        clf = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=LogisticRegression(max_iter=500, random_state=42),\n",
    "            cv=3\n",
    "        )\n",
    "    else:\n",
    "        # Simple voting\n",
    "        if len(estimators) == 1:\n",
    "            clf = estimators[0][1]   # just the single model\n",
    "        else:\n",
    "            clf = VotingClassifier(estimators=estimators, voting=\"soft\")\n",
    "    \n",
    "    # Train & evaluate\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f514c7-aa26-4e6e-8ffc-36f192cb5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: evaluate Digits dataset with action 3 (xgb+rf)\n",
    "reward = evaluate_action(X_digits.values, y_digits, action_id=3, action_map=action_map)\n",
    "print(\"Reward (Validation Accuracy):\", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f918b-df11-4312-89ac-6e5d800ffe1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Building Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d531896-4b6a-4c8b-bbbf-6f7844ac5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc73316-5d1e-4303-a76e-ff27bbfb1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class ModelSelectionEnv(gym.Env):\n",
    "    def __init__(self, datasets, action_map, state_keys=None):\n",
    "        super(ModelSelectionEnv, self).__init__()\n",
    "        \n",
    "        self.datasets = datasets\n",
    "        self.action_map = action_map\n",
    "        self.state_keys = state_keys or [\n",
    "            \"n1_borderline\", \"n2_ratio\", \"n3_error\",\n",
    "            \"l2_linear_sep\", \"l3_nonlinear_sep\", \"f1_feature_eff\",\n",
    "            \"num_classes\", \"num_features\", \"num_samples\"\n",
    "        ]\n",
    "        \n",
    "        self.action_space = spaces.Discrete(len(action_map))\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, \n",
    "                                            shape=(len(self.state_keys),), dtype=np.float32)\n",
    "        self.current_dataset = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        import random\n",
    "        self.current_dataset = random.choice(self.datasets)\n",
    "        X, y, measures = self.current_dataset\n",
    "        self.state = np.array([measures[k] for k in self.state_keys], dtype=float)\n",
    "        return self.state, {}\n",
    "\n",
    "\n",
    "    def step(self, action_id):\n",
    "        X, y, measures = self.current_dataset\n",
    "        reward = evaluate_action(X, y, action_id, self.action_map)\n",
    "        done = True\n",
    "        return self.state, reward, done, False, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af14dd-7c89-4bd3-a471-b55cade26b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (X_iris.values, y_iris_encoded, iris_measures),\n",
    "    (X_digits.values, y_digits, digits_measures)\n",
    "]\n",
    "\n",
    "env = ModelSelectionEnv(datasets, action_map)\n",
    "\n",
    "# Run one episode manually\n",
    "state, info = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "next_state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "print(\"Action taken:\", action, \"Reward:\", reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd7a8d-3850-4c52-9c7c-5622326b86df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf04ce8-9193-4561-9e1f-dcd37fa8c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (X_iris.values, y_iris_encoded, iris_measures),\n",
    "    (X_digits.values, y_digits, digits_measures),\n",
    "    (X_rcv1_reduced, y_rcv1_small, rcv1_measures),\n",
    "    (X_20news_reduced, y_20news_small, news_measures),\n",
    "    (X_c3f_small.values, y_cat_small, cat_measures),      # IoT Cat\n",
    "    (X_c3f_small.values, y_sub_small, sub_measures),      # IoT Sub_Cat\n",
    "    (X_c3f_small.values, y_label_small, label_measures)   # IoT Label\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e16e7-1745-4456-a7b0-03f8c4b5ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModelSelectionEnv(datasets, action_map)\n",
    "\n",
    "# Run one random episode\n",
    "state, info = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "next_state, reward, done, trunc, _ = env.step(action)\n",
    "print(\"Action taken:\", action, \"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af5c55-a827-4b7a-b7ce-0da4706502d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Choose RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e657c-5037-4aaf-8e9d-2d2ce276a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Original (subset + reduce)\n",
    "X_rcv1_small = X_rcv1[:1000].toarray()\n",
    "y_rcv1_small = y_rcv1_single[:1000]\n",
    "\n",
    "# Encode labels to 0..N-1\n",
    "le_rcv1 = LabelEncoder()\n",
    "y_rcv1_encoded = le_rcv1.fit_transform(y_rcv1_small)\n",
    "\n",
    "print(\"RCV1 Encoded classes:\", np.unique(y_rcv1_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9572d0-4856-4bb0-980e-f4df3fd185ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    prepare_dataset(X_iris.values, y_iris_encoded, iris_measures),\n",
    "    prepare_dataset(X_digits.values, y_digits, digits_measures),\n",
    "    (X_rcv1_small, y_rcv1_encoded, rcv1_measures),     # <— use fixed encoded labels\n",
    "    prepare_dataset(X_20news_reduced, y_20news_small, news_measures),\n",
    "    prepare_dataset(X_c3f_small.values, y_cat_small, cat_measures),\n",
    "    prepare_dataset(X_c3f_small.values, y_sub_small, sub_measures),\n",
    "    prepare_dataset(X_c3f_small.values, y_label_small, label_measures)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c05336-bc6f-4b6e-a001-1a08d49116de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinUCBAgent:\n",
    "    def __init__(self, n_actions, n_features, alpha=1.0):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # For each action, track matrix A (dxd) and vector b (dx1)\n",
    "        self.A = [np.identity(n_features) for _ in range(n_actions)]\n",
    "        self.b = [np.zeros((n_features, 1)) for _ in range(n_actions)]\n",
    "    \n",
    "    def select_action(self, context):\n",
    "        \"\"\"\n",
    "        Pick action using UCB score: p = θ^T x + α * sqrt(x^T A^-1 x)\n",
    "        \"\"\"\n",
    "        context = context.reshape(-1, 1)\n",
    "        p_values = []\n",
    "        \n",
    "        for a in range(self.n_actions):\n",
    "            A_inv = np.linalg.inv(self.A[a])\n",
    "            theta = A_inv @ self.b[a]\n",
    "            mean = float(theta.T @ context)\n",
    "            conf = self.alpha * np.sqrt(float(context.T @ A_inv @ context))\n",
    "            p = mean + conf\n",
    "            p_values.append(p)\n",
    "        \n",
    "        return np.argmax(p_values)\n",
    "    \n",
    "    def update(self, action, context, reward):\n",
    "        \"\"\"\n",
    "        Update action-specific A and b with new observation\n",
    "        \"\"\"\n",
    "        context = context.reshape(-1, 1)\n",
    "        self.A[action] += context @ context.T\n",
    "        self.b[action] += reward * context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df301a-7604-4cf1-baf3-bf8c3251346e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94cb73-4447-45c3-96d5-5dd7f84cd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = len(action_map)\n",
    "n_features = len(state)   # state is one meta-feature vector (len=9)\n",
    "\n",
    "agent = LinUCBAgent(n_actions, n_features, alpha=1.0)\n",
    "\n",
    "n_episodes = 50\n",
    "for ep in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    action = agent.select_action(state)\n",
    "    _, reward, done, trunc, _ = env.step(action)\n",
    "    agent.update(action, state, reward)\n",
    "    print(f\"Episode {ep+1}: Action={action}, Reward={reward:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f2b1d-0cfe-4346-b0a8-e254c1a1f6ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluating baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df3e94-4efb-4961-abfb-01cccaa96985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Fix RCV1 labels ---\n",
    "le_rcv1 = LabelEncoder()\n",
    "y_rcv1_encoded = le_rcv1.fit_transform(y_rcv1_small)\n",
    "print(\"RCV1 encoded classes:\", np.unique(y_rcv1_encoded))\n",
    "\n",
    "# --- Fix 20News labels (just in case) ---\n",
    "le_news = LabelEncoder()\n",
    "y_20news_encoded = le_news.fit_transform(y_20news_small)\n",
    "print(\"20News encoded classes:\", np.unique(y_20news_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d5a9f-fc0c-499a-986a-efa1a7ffb519",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    prepare_dataset(clean_features(X_iris.values), y_iris_encoded, iris_measures),\n",
    "    prepare_dataset(clean_features(X_digits.values), y_digits, digits_measures),\n",
    "    (clean_features(X_rcv1_small), y_rcv1_encoded, rcv1_measures),\n",
    "    (clean_features(X_20news_reduced), y_20news_encoded, news_measures),\n",
    "    prepare_dataset(clean_features(X_c3f_small.values), y_cat_small, cat_measures),\n",
    "    prepare_dataset(clean_features(X_c3f_small.values), y_sub_small, sub_measures),\n",
    "    prepare_dataset(clean_features(X_c3f_small.values), y_label_small, label_measures)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19327206-e615-467d-8ffc-7ce3a0a3bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModelSelectionEnv(datasets, action_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f2287-4f6a-4403-beaf-6c33b1c00c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# Random Policy Agent (baseline)\n",
    "# ----------------------------\n",
    "class RandomAgent:\n",
    "    def __init__(self, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def select_action(self, context):\n",
    "        return random.randint(0, self.n_actions - 1)\n",
    "    \n",
    "    def update(self, action, context, reward):\n",
    "        pass  # no learning for random agent\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop (Generic)\n",
    "# ----------------------------\n",
    "def run_agent(env, agent, n_episodes=100):\n",
    "    rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        state, info = env.reset()\n",
    "        action = agent.select_action(state)\n",
    "        _, reward, done, trunc, _ = env.step(action)\n",
    "        agent.update(action, state, reward)\n",
    "        rewards.append(reward)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a62815-dd04-42fc-9043-7235e3da1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = len(action_map)\n",
    "n_features = len(state)   # length of one meta-feature vector\n",
    "\n",
    "linucb_agent = LinUCBAgent(n_actions, n_features, alpha=1.0)\n",
    "random_agent = RandomAgent(n_actions)\n",
    "\n",
    "n_episodes = 100\n",
    "linucb_rewards = run_agent(env, linucb_agent, n_episodes)\n",
    "random_rewards = run_agent(env, random_agent, n_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513a5d4-9d67-408f-8b8c-3bea2c9272f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.cumsum(linucb_rewards) / (np.arange(len(linucb_rewards)) + 1),\n",
    "         label=\"LinUCB (avg reward)\", linewidth=2)\n",
    "plt.plot(np.cumsum(random_rewards) / (np.arange(len(random_rewards)) + 1),\n",
    "         label=\"Random Policy (avg reward)\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward (Validation Accuracy)\")\n",
    "plt.title(\"LinUCB vs Random Baseline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9aef60-7b15-41da-8863-f081990288d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236ef80-5328-4931-aa62-ea2cd28fd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slimmed-down re-run for tuning alpha\n",
    "alphas = [0.1, 0.5]\n",
    "results = {}\n",
    "\n",
    "for a in alphas:\n",
    "    linucb_agent = LinUCBAgent(n_actions, n_features, alpha=a)  # reinit agent\n",
    "    rewards = run_agent(env, linucb_agent, n_episodes=100)      # rerun training\n",
    "    results[a] = rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ace03-daba-4940-a2fd-dae787819126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vs random baseline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "for a, rewards in results.items():\n",
    "    plt.plot(np.cumsum(rewards) / (np.arange(len(rewards)) + 1), label=f\"LinUCB α={a}\")\n",
    "\n",
    "plt.plot(np.cumsum(random_rewards) / (np.arange(len(random_rewards)) + 1),\n",
    "         label=\"Random Baseline\", linestyle=\"--\", color=\"black\")\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Tuning LinUCB α vs Random\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d16de3-9b01-4e42-a11f-c8a990a050ae",
   "metadata": {},
   "source": [
    "## Metrics report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5f6ae-9f71-4fa8-ab0d-a4019a219246",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(linucb_rewards) #Average Reward (mean validation accuracy across episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168bac3-1425-431f-ad1c-568bacf2d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(linucb_rewards) #Best Reward (max accuracy achieved in any episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5d8ca-e5eb-45f1-8211-274c0df8ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(linucb_rewards) #Stability (standard deviation of rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048af75e-fa80-4608-9317-45f87a5c4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.1, 0.5, 1.0]\n",
    "results = {}\n",
    "\n",
    "for a in alphas:\n",
    "    linucb_agent = LinUCBAgent(n_actions, n_features, alpha=a)\n",
    "    rewards = run_agent(env, linucb_agent, n_episodes=100)\n",
    "    results[a] = {\n",
    "        \"Avg Reward\": np.mean(rewards),\n",
    "        \"Max Reward\": np.max(rewards),\n",
    "        \"Std Dev\": np.std(rewards)\n",
    "    }\n",
    "\n",
    "# Add Random baseline too\n",
    "random_agent = RandomAgent(n_actions)\n",
    "random_rewards = run_agent(env, random_agent, n_episodes=100)\n",
    "results[\"Random\"] = {\n",
    "    \"Avg Reward\": np.mean(random_rewards),\n",
    "    \"Max Reward\": np.max(random_rewards),\n",
    "    \"Std Dev\": np.std(random_rewards)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame(results).T\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b9d42-aee1-4612-a95d-5af442530f7b",
   "metadata": {},
   "source": [
    "Avg Reward  Max Reward   Std Dev\n",
    "0.1       0.668756    0.972222  0.277453\n",
    "0.5       0.695589    0.972222  0.279794\n",
    "1.0       0.633250    0.966667  0.296364\n",
    "Random    0.684150    0.972222  0.270051"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4babf5-98f5-4769-be13-1a89bb26efb0",
   "metadata": {},
   "source": [
    "## Applying RL agaent to real-life(iot) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23dfc8-382d-4f1a-a871-36c59beb7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "c3f=pd.read_csv(r\"C:\\Users\\Kaveri\\Downloads\\CHASE datasets\\c3f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700a6a1-5480-4cbe-a128-7a45a105752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3f['Label_num'] = c3f['Label'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7d6c1-169f-4a55-a826-ec94765877c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: Train (70%) and temp (30%)\n",
    "train_df, temp_df = train_test_split(\n",
    "    c3f,\n",
    "    test_size=0.30,\n",
    "    stratify=c3f['Label_num'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Validation (15%) and Test (15%) from temp\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,   # Half of 30% = 15%\n",
    "    stratify=temp_df['Label_num'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Validation shape: {val_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718139cf-b983-41a4-83d4-5ac38794de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train label distribution:\\n\", train_df['Label'].value_counts(normalize=True))\n",
    "print(\"Validation label distribution:\\n\", val_df['Label'].value_counts(normalize=True))\n",
    "print(\"Test label distribution:\\n\", test_df['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee4798-cd99-417f-9d94-3d90f72fb535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0fef1b-f4dc-41a2-aa47-03c2f97cf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# 2️⃣ Encode label for stratification\n",
    "c3f['Label_num'] = c3f['Label'].astype('category').cat.codes\n",
    "\n",
    "# 3️⃣ Split 70-15-15\n",
    "train_df, temp_df = train_test_split(\n",
    "    c3f,\n",
    "    test_size=0.30,\n",
    "    stratify=c3f['Label_num'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_df['Label_num'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Validation shape: {val_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# 4️⃣ Cleaning function (no scaling yet)\n",
    "def clean_features(df):\n",
    "    features = df.drop(columns=['Cat', 'Sub_Cat', 'Label', 'Label_num'])\n",
    "    features_numeric = features.select_dtypes(include=['int64', 'float64'])\n",
    "    features_clean = features_numeric.copy()\n",
    "    features_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    features_clean.fillna(features_clean.median(), inplace=True)\n",
    "    return features_clean\n",
    "\n",
    "X_train_clean = clean_features(train_df)\n",
    "X_val_clean = clean_features(val_df)\n",
    "X_test_clean = clean_features(test_df)\n",
    "\n",
    "# 5️⃣ Scaling: fit on train, transform on val/test\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_val_scaled = scaler.transform(X_val_clean)\n",
    "X_test_scaled = scaler.transform(X_test_clean)\n",
    "\n",
    "# Convert back to DataFrames for feature names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_clean.columns, index=X_train_clean.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val_clean.columns, index=X_val_clean.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_clean.columns, index=X_test_clean.index)\n",
    "\n",
    "# 6️⃣ Mutual Information Feature Selection (on TRAIN ONLY, leakage-free)\n",
    "feature_cols = X_train_scaled.columns.tolist()\n",
    "targets = ['Cat', 'Sub_Cat', 'Label']\n",
    "mi_results = {}\n",
    "\n",
    "for target in targets:\n",
    "    y_target = train_df[target]\n",
    "    mi = mutual_info_classif(X_train_scaled, y_target, discrete_features='auto', random_state=42)\n",
    "    mi_series = pd.Series(mi, index=feature_cols).sort_values(ascending=False)\n",
    "    mi_results[target] = mi_series\n",
    "    print(f\"\\n🔹 Top features for {target}:\\n\", mi_series.head(20))\n",
    "\n",
    "# 7️⃣ Build final feature lists per level\n",
    "cat_features = mi_results['Cat'].head(20).index.tolist()\n",
    "subcat_features = mi_results['Sub_Cat'].head(20).index.tolist()\n",
    "label_features = mi_results['Label'].head(20).index.tolist()\n",
    "\n",
    "cross_level_features = list(set(cat_features) & set(subcat_features) & set(label_features))\n",
    "\n",
    "print(\"\\n🔹 Cross-level features:\\n\", cross_level_features)\n",
    "\n",
    "cat_final_features = list(set(cat_features + cross_level_features))\n",
    "subcat_final_features = list(set(subcat_features + cross_level_features))\n",
    "label_final_features = list(set(label_features + cross_level_features))\n",
    "\n",
    "print(f\"\\n✅ Final Cat features ({len(cat_final_features)}):\\n\", cat_final_features)\n",
    "print(f\"\\n✅ Final Sub_Cat features ({len(subcat_final_features)}):\\n\", subcat_final_features)\n",
    "print(f\"\\n✅ Final Label features ({len(label_final_features)}):\\n\", label_final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06323c-c496-4369-beb8-fe521103588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_determinism(df, target_col, previous_target_cols):\n",
    "    \"\"\"\n",
    "    Check if target_col is perfectly determined by previous_target_cols.\n",
    "    Returns True if deterministic, False otherwise.\n",
    "    \"\"\"\n",
    "    if not previous_target_cols:\n",
    "        return False\n",
    "\n",
    "    # Build key from previous target(s)\n",
    "    if len(previous_target_cols) == 1:\n",
    "        combo_key = df[previous_target_cols[0]].astype(str)\n",
    "    else:\n",
    "        combo_key = df[previous_target_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "\n",
    "    mapping = {}\n",
    "    for combo, target in zip(combo_key, df[target_col]):\n",
    "        if combo in mapping:\n",
    "            if mapping[combo] != target:  # conflict found\n",
    "                return False\n",
    "        else:\n",
    "            mapping[combo] = target\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# 👉 Check determinism in IoT dataset\n",
    "print(\"Is Sub_Cat deterministic from Cat?\",\n",
    "      check_determinism(c3f, \"Sub_Cat\", [\"Cat\"]))\n",
    "\n",
    "print(\"Is Label deterministic from (Cat, Sub_Cat)?\",\n",
    "      check_determinism(c3f, \"Label\", [\"Cat\", \"Sub_Cat\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8758ec-1295-4fb7-aebc-61569ed85fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Extract X and y for each level\n",
    "X_cat = X_train_scaled[cat_final_features]\n",
    "y_cat = train_df['Cat']\n",
    "\n",
    "X_sub = X_train_scaled[subcat_final_features]\n",
    "y_sub = train_df['Sub_Cat']\n",
    "\n",
    "X_label = X_train_scaled[label_final_features]\n",
    "y_label = train_df['Label']\n",
    "\n",
    "# 2️⃣ Compute complexity measures for each target level\n",
    "cat_measures = analyzer.calculate_all_measures(X_cat, y_cat, target_name=\"IoT Cat\")\n",
    "subcat_measures = analyzer.calculate_all_measures(X_sub, y_sub, target_name=\"IoT SubCat\")\n",
    "label_measures = analyzer.calculate_all_measures(X_label, y_label, target_name=\"IoT Label\")\n",
    "\n",
    "# 3️⃣ Prepare IoT datasets for RL environment\n",
    "iot_datasets = [\n",
    "    prepare_dataset(X_cat, y_cat, cat_measures),\n",
    "    prepare_dataset(X_sub, y_sub, subcat_measures),\n",
    "    prepare_dataset(X_label, y_label, label_measures),\n",
    "]\n",
    "\n",
    "# 4️⃣ Build IoT-only environment\n",
    "iot_env = ModelSelectionEnv(iot_datasets, action_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a82351-0ca2-4bf5-9777-9c3daf52ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LinUCB Test Run on IoT Dataset\n",
    "# ----------------------------\n",
    "\n",
    "# Prepare IoT datasets (skip Label if deterministic)\n",
    "iot_datasets = [\n",
    "    prepare_dataset(X_cat, y_cat, cat_measures),\n",
    "    prepare_dataset(X_sub, y_sub, subcat_measures),\n",
    "]\n",
    "\n",
    "if not check_determinism(c3f, \"Label\", [\"Cat\", \"Sub_Cat\"]):\n",
    "    iot_datasets.append(prepare_dataset(X_label, y_label, label_measures))\n",
    "else:\n",
    "    print(\"⚡ Label is deterministic from (Cat, Sub_Cat) → skipping RL for Label.\")\n",
    "\n",
    "# Build IoT environment\n",
    "iot_env = ModelSelectionEnv(iot_datasets, action_map)\n",
    "\n",
    "# ----------------------------\n",
    "# Run LinUCB on IoT\n",
    "# ----------------------------\n",
    "n_actions = len(action_map)\n",
    "\n",
    "# FIX: reset once to get feature vector length\n",
    "state, info = iot_env.reset()\n",
    "n_features = len(state)\n",
    "\n",
    "linucb_agent = LinUCBAgent(n_actions, n_features, alpha=0.5)  # you can tune alpha\n",
    "\n",
    "n_episodes = 50\n",
    "iot_rewards = []\n",
    "iot_actions = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, info = iot_env.reset()\n",
    "    action = linucb_agent.select_action(state)\n",
    "    _, reward, done, trunc, _ = iot_env.step(action)\n",
    "    linucb_agent.update(action, state, reward)\n",
    "    iot_rewards.append(reward)\n",
    "    iot_actions.append(action)\n",
    "    print(f\"Episode {ep+1}: Action={action_map[action]}, Reward={reward:.3f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Summarize results\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "\n",
    "summary = pd.Series(iot_actions).map(action_map).value_counts()\n",
    "print(\"\\n📊 Model selection frequency on IoT dataset:\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n✅ Average Reward:\", np.mean(iot_rewards))\n",
    "print(\"✅ Best Reward:\", np.max(iot_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94e6ef-8733-4fa7-ae00-2d9250e557ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Level wise model reccommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b071bb1-68f4-45f5-aba9-9fdb52756c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: Run LinUCB on a given env\n",
    "# ----------------------------\n",
    "def run_linucb_on_env(env, n_episodes=50, alpha=0.5):\n",
    "    state, info = env.reset()\n",
    "    n_features = len(state)\n",
    "    n_actions = len(action_map)\n",
    "\n",
    "    agent = LinUCBAgent(n_actions, n_features, alpha=alpha)\n",
    "    rewards, actions = [], []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        state, info = env.reset()\n",
    "        action = agent.select_action(state)\n",
    "        _, reward, done, trunc, _ = env.step(action)\n",
    "        agent.update(action, state, reward)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "\n",
    "    return rewards, actions\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ CAT ENVIRONMENT\n",
    "# ----------------------------\n",
    "iot_env_cat = ModelSelectionEnv(\n",
    "    [prepare_dataset(X_cat, y_cat, cat_measures)], action_map\n",
    ")\n",
    "cat_rewards, cat_actions = run_linucb_on_env(iot_env_cat, n_episodes=50, alpha=0.5)\n",
    "\n",
    "print(\"\\n================ CAT RESULTS ================\")\n",
    "print(pd.Series(cat_actions).map(action_map).value_counts())\n",
    "print(\"✅ Avg Reward (Val Acc):\", np.mean(cat_rewards))\n",
    "print(\"✅ Best Reward (Val Acc):\", np.max(cat_rewards))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2️⃣ SUBCAT ENVIRONMENT\n",
    "# ----------------------------\n",
    "iot_env_sub = ModelSelectionEnv(\n",
    "    [prepare_dataset(X_sub, y_sub, subcat_measures)], action_map\n",
    ")\n",
    "sub_rewards, sub_actions = run_linucb_on_env(iot_env_sub, n_episodes=50, alpha=0.5)\n",
    "\n",
    "print(\"\\n================ SUBCAT RESULTS ================\")\n",
    "print(pd.Series(sub_actions).map(action_map).value_counts())\n",
    "print(\"✅ Avg Reward (Val Acc):\", np.mean(sub_rewards))\n",
    "print(\"✅ Best Reward (Val Acc):\", np.max(sub_rewards))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3️⃣ LABEL CHECK\n",
    "# ----------------------------\n",
    "if check_determinism(c3f, \"Label\", [\"Cat\", \"Sub_Cat\"]):\n",
    "    print(\"\\n================ LABEL RESULTS ================\")\n",
    "    print(\"⚡ Label is deterministic from (Cat, Sub_Cat).\")\n",
    "    print(\"✅ No RL needed → Rule-based classification = 100% accuracy.\")\n",
    "else:\n",
    "    iot_env_label = ModelSelectionEnv(\n",
    "        [prepare_dataset(X_label, y_label, label_measures)], action_map\n",
    "    )\n",
    "    label_rewards, label_actions = run_linucb_on_env(iot_env_label, n_episodes=50, alpha=0.5)\n",
    "    print(\"\\n================ LABEL RESULTS ================\")\n",
    "    print(pd.Series(label_actions).map(action_map).value_counts())\n",
    "    print(\"✅ Avg Reward (Val Acc):\", np.mean(label_rewards))\n",
    "    print(\"✅ Best Reward (Val Acc):\", np.max(label_rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f87c-f676-4e96-98a4-8eea3d0e03c8",
   "metadata": {},
   "source": [
    "================ CAT RESULTS ================\n",
    "{'models': ['LightGBM', 'RandomForest'], 'meta': None}                               5\n",
    "{'models': ['XGBoost', 'LightGBM', 'RandomForest'], 'meta': None}                    5\n",
    "{'models': ['XGBoost', 'LightGBM', 'RandomForest'], 'meta': 'LogisticRegression'}    5\n",
    "{'models': ['LightGBM', 'RandomForest'], 'meta': 'LogisticRegression'}               5\n",
    "{'models': ['XGBoost', 'LightGBM'], 'meta': None}                                    4\n",
    "{'models': ['XGBoost'], 'meta': None}                                                4\n",
    "{'models': ['LightGBM'], 'meta': None}                                               4\n",
    "{'models': ['XGBoost', 'RandomForest'], 'meta': None}                                4\n",
    "{'models': ['XGBoost', 'LightGBM'], 'meta': 'LogisticRegression'}                    4\n",
    "{'models': ['XGBoost', 'RandomForest'], 'meta': 'LogisticRegression'}                4\n",
    "{'models': ['RandomForest'], 'meta': None}                                           4\n",
    "{'models': ['LogisticRegression'], 'meta': None}                                     2\n",
    "Name: count, dtype: int64\n",
    "✅ Avg Reward (Val Acc): 0.9754124679760888\n",
    "✅ Best Reward (Val Acc): 0.9826033914846896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f3baa-0fe7-4706-bae3-43234c90214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "================ SUBCAT RESULTS ================\n",
    "{'models': ['XGBoost', 'LightGBM', 'RandomForest'], 'meta': 'LogisticRegression'}    7\n",
    "{'models': ['XGBoost', 'RandomForest'], 'meta': 'LogisticRegression'}                6\n",
    "{'models': ['LightGBM', 'RandomForest'], 'meta': 'LogisticRegression'}               6\n",
    "{'models': ['XGBoost', 'LightGBM'], 'meta': 'LogisticRegression'}                    5\n",
    "{'models': ['XGBoost', 'LightGBM'], 'meta': None}                                    4\n",
    "{'models': ['XGBoost'], 'meta': None}                                                4\n",
    "{'models': ['LightGBM'], 'meta': None}                                               4\n",
    "{'models': ['XGBoost', 'RandomForest'], 'meta': None}                                3\n",
    "{'models': ['LightGBM', 'RandomForest'], 'meta': None}                               3\n",
    "{'models': ['XGBoost', 'LightGBM', 'RandomForest'], 'meta': None}                    3\n",
    "{'models': ['RandomForest'], 'meta': None}                                           3\n",
    "{'models': ['LogisticRegression'], 'meta': None}                                     2\n",
    "Name: count, dtype: int64\n",
    "✅ Avg Reward (Val Acc): 0.8579397340490423\n",
    "✅ Best Reward (Val Acc): 0.907844333292668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0521d7-c0a1-43eb-ac9c-524704b9b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/val/test splits to CSV for AutoML\n",
    "train_df.to_csv(\"iot_train.csv\", index=False)\n",
    "val_df.to_csv(\"iot_val.csv\", index=False)\n",
    "test_df.to_csv(\"iot_test.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved train/val/test splits as CSVs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e1575-51e7-4733-874d-d0670a72d4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf2843-c0ac-44b3-81fb-093fe2d4c5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecceabb5-9701-439d-a4cd-9cd5593de0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcd8a0-1d28-4c62-ad28-f7517a69939c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1612e8-e069-40b3-85a3-8dbb0e311dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b4948-78ca-470e-a512-934313c64f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2754bf5-4a1d-4755-af09-09388d69dd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
